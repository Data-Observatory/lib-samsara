{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8665719a",
   "metadata": {},
   "source": [
    "## Modelamiento de cambios estructurales para detección de cambios en bosques y matorrales de Chile central usando series de tiempo de datos NDVI Landsat-5, -7, -8, y -9\n",
    "\n",
    "### Este Notebook muestra el uso del modelo Pruned Exact Linear Time (PELT) \n",
    "\n",
    "- **Interfáz**: Data Cube Chile (DCC)\n",
    "- **Lenguaje**: Python\n",
    "- **Última actualización**: Julio 2023\n",
    "- **Autor**: Ignacio fuentes San Roman \\ ignacio.fuentes.sanroman@gmail.com \\ Universidad de las Americas (UDLA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5803d7-92a4-43a9-808a-6bc07d4b2597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datacube\n",
    "import time\n",
    "import datetime\n",
    "from multiprocessing import Pool\n",
    "import itertools\n",
    "import rasterio\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from datacube.utils import masking\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from odc.ui import DcViewer\n",
    "from odc.ui import with_ui_cbk\n",
    "\n",
    "import ruptures as rpt # PELT library\n",
    "\n",
    "\n",
    "# Set some configurations for displaying tables nicely\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(1, '../Tools/') # poner acá la dirección de la carpeta dea_tools de la repo dea-notebooks. Clonar desde https://github.com/GeoscienceAustralia/dea-notebooks.git\n",
    "from dea_tools.plotting import display_map, rgb\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c654e2f5-8db9-4ebf-95b2-b5d43515fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de créditos AWS y Dask\n",
    "from datacube.utils.rio import configure_s3_access\n",
    "from dask.distributed import Client\n",
    "\n",
    "configure_s3_access(aws_unsigned=False, requester_pays=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e124ec1",
   "metadata": {},
   "source": [
    "### Funciones a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ffc48-17df-453b-8699-0ac36db3aa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciones para obtener fechas de detección de cambios\n",
    "\n",
    "def get_days(df):\n",
    "    '''\n",
    "    Converts breaks to days from 2000-01-01\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "        DataFrame containing the dates\n",
    "    \n",
    "    Returns:\n",
    "    - lambda function\n",
    "        A lambda function that calculates the number of days from 2000-01-01 for a given index\n",
    "    '''\n",
    "    return lambda x: (df.loc[x]['index'] - pd.to_datetime('2000-01-01')).days + 0.0 if x > 0 else np.nan\n",
    "\n",
    "    \n",
    "def bkps2dates(array, pena):\n",
    "    '''\n",
    "    Converts breakpoints to dates\n",
    "    \n",
    "    Parameters:\n",
    "    - array: numpy array\n",
    "        Array containing the breakpoints\n",
    "    - pena: int\n",
    "        Penalty value\n",
    "    \n",
    "    Returns:\n",
    "    - float\n",
    "        The index of the second-to-last breakpoint in the array\n",
    "    '''\n",
    "    try:\n",
    "        ixs = len(array)\n",
    "        df_array = pd.DataFrame(data={'ndvi':array.ravel(), 'ix':np.array(range(ixs))})\n",
    "        df_array2 = df_array.dropna(how='any')\n",
    "        model = 'rbf' #\"l1\", \"l2\", \"rbf\"\n",
    "        algo = ruptures.KernelCPD(kernel=model, min_size=3, jump=5).fit(df_array2['ndvi'].values)\n",
    "        my_bkps = algo.predict(pen=20)\n",
    "        if len(my_bkps) > 1:\n",
    "            return df_array2.reset_index().iloc[my_bkps[-2]-1]['ix']\n",
    "        else:\n",
    "            return np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def create_df(data, dates):\n",
    "    \"\"\"\n",
    "    Create a pandas DataFrame from the given data and dates.\n",
    "\n",
    "    Parameters:\n",
    "    - data: numpy array\n",
    "        Array containing the data values\n",
    "    - dates: list\n",
    "        List of dates corresponding to the data values\n",
    "\n",
    "    Returns:\n",
    "    - df: pandas DataFrame\n",
    "        DataFrame with 'ndvi' and 'ix' columns\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(index=pd.to_datetime(dates),\n",
    "                      data={'ndvi': data.ravel(), 'ix': range(len(data))})\n",
    "    df = df.dropna(how='any')\n",
    "    return df\n",
    "\n",
    "\n",
    "def model_pelt(df, pen):\n",
    "    \"\"\"\n",
    "    Apply the Pruned Exact Linear Time (PELT) algorithm to the given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "        DataFrame containing the data\n",
    "    - pen: int\n",
    "        Penalty value for the PELT algorithm\n",
    "\n",
    "    Returns:\n",
    "    - my_bkps: list\n",
    "        List of breakpoints detected by the PELT algorithm\n",
    "    \"\"\"\n",
    "    model = 'rbf'  # \"l1\", \"l2\", \"rbf\"\n",
    "    algo = rpt.KernelCPD(kernel=model, min_size=3, jump=5).fit(df['ndvi'].values)\n",
    "    my_bkps = algo.predict(pen=pen)\n",
    "    my_bkps = my_bkps[:-1]\n",
    "    my_bkps = [n - 1 for n in my_bkps]\n",
    "    return my_bkps\n",
    "\n",
    "\n",
    "def bkps2dates2(array, df_dates):\n",
    "    \"\"\"\n",
    "    Convert breakpoints to dates and calculate corresponding values.\n",
    "\n",
    "    Parameters:\n",
    "    - array: numpy array\n",
    "        Array containing the data values\n",
    "    - df_dates: pandas DataFrame\n",
    "        DataFrame containing the dates\n",
    "\n",
    "    Returns:\n",
    "    - result: numpy array\n",
    "        Array containing the dates, breakpoints, and corresponding values\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = create_df(array, df_dates)\n",
    "        my_bkps = model_pelt(df, 20)\n",
    "        df3 = df.reset_index()\n",
    "        df3 = df3[df3['index'] > '2016-01-01']\n",
    "        my_bkps = sorted(list(set(my_bkps).intersection(df3.index.tolist())))\n",
    "\n",
    "        if len(df3.loc[my_bkps]) >= 1:\n",
    "            days = [df3.loc[n]['index'].year + ((df3.loc[n]['index'] - datetime.datetime(df3.loc[n]['index'].year, 1, 1)).days / 365) for n in my_bkps]\n",
    "            arr = np.full((5, 1), np.nan)\n",
    "            arr[:len(days)] = np.array(days).reshape(-1, 1)\n",
    "            bks = [df3.loc[n]['ix'] for n in my_bkps]\n",
    "            a = np.full((5, 1), np.nan)\n",
    "            a[:len(bks)] = np.array(bks).reshape(-1, 1)\n",
    "            ndvis = []\n",
    "\n",
    "            if len(my_bkps) == 1:\n",
    "                ndvis.append(df.iloc[my_bkps[0]:]['ndvi'].mean() - df.iloc[:my_bkps[0]]['ndvi'].mean())\n",
    "            elif len(my_bkps) == 2:\n",
    "                ndvis.append(df.iloc[my_bkps[0]:my_bkps[1]]['ndvi'].mean() - df.iloc[:my_bkps[0]]['ndvi'].mean())\n",
    "                ndvis.append(df.iloc[my_bkps[1]:]['ndvi'].mean() - df.iloc[my_bkps[0]:my_bkps[1]]['ndvi'].mean())\n",
    "            elif len(my_bkps) == 3:\n",
    "                ndvis.append(df.iloc[my_bkps[0]:my_bkps[1]]['ndvi'].mean() - df.iloc[:my_bkps[0]]['ndvi'].mean())\n",
    "                ndvis.append(df.iloc[my_bkps[1]:my_bkps[2]]['ndvi'].mean() - df.iloc[my_bkps[0]:my_bkps[1]]['ndvi'].mean())\n",
    "                ndvis.append(df.iloc[my_bkps[2]:]['ndvi'].mean() - df.iloc[my_bkps[1]:my_bkps[2]]['ndvi'].mean())\n",
    "            elif len(my_bkps) == 4:\n",
    "                ndvis.append(df.iloc[my_bkps[0]:my_bkps[1]]['ndvi'].mean() - df.iloc[:my_bkps[0]]['ndvi'].mean())\n",
    "                ndvis.append(df.iloc[my_bkps[1]:my_bkps[2]]['ndvi'].mean() - df.iloc[my_bkps[0]:my_bkps[1]]['ndvi'].mean())\n",
    "                ndvis.append(df.iloc[my_bkps[2]:my_bkps[3]]['ndvi'].mean() - df.iloc[my_bkps[1]:my_bkps[2]]['ndvi'].mean())\n",
    "                ndvis.append(df.iloc[my_bkps[3]:]['ndvi'].mean() - df.iloc[my_bkps[2]:my_bkps[3]]['ndvi'].mean())\n",
    "            else:\n",
    "                ndvis.append(df.iloc[my_bkps[0]:my_bkps[1]]['ndvi'].mean() - df.iloc[:my_bkps[0]]['ndvi'].mean())\n",
    "                ndvis.append(df.iloc[my_bkps[1]:my_bkps[2]]['ndvi'].mean() - df.iloc[my_bkps[0]:my_bkps[1]]['ndvi'].mean())\n",
    "                ndvis.append(df.iloc[my_bkps[2]:my_bkps[3]]['ndvi'].mean() - df.iloc[my_bkps[1]:my_bkps[2]]['ndvi'].mean())\n",
    "                ndvis.append(df.iloc[my_bkps[3]:my_bkps[4]]['ndvi'].mean() - df.iloc[my_bkps[2]:my_bkps[3]]['ndvi'].mean())\n",
    "                ndvis.append(df.iloc[my_bkps[4]:]['ndvi'].mean() - df.iloc[my_bkps[3]:my_bkps[4]]['ndvi'].mean())\n",
    "            mag = np.full((5, 1), np.nan)\n",
    "            mag[:len(bks)] = np.array(ndvis).reshape(-1, 1)\n",
    "            return np.hstack([arr.astype(float).transpose(),\n",
    "                              a.astype(float).transpose(),\n",
    "                              mag.astype(float).transpose()])\n",
    "        else:\n",
    "            return np.hstack([np.full((5, 1), np.nan).transpose(),\n",
    "                              np.full((5, 1), np.nan).transpose(),\n",
    "                              np.full((5, 1), np.nan).transpose()])\n",
    "    except:\n",
    "        return np.hstack([np.full((5, 1), np.nan).transpose(),\n",
    "                          np.full((5, 1), np.nan).transpose(),\n",
    "                          np.full((5, 1), np.nan).transpose()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bbf330",
   "metadata": {},
   "source": [
    "### Cargar y limpiar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97b4ce3-bc5c-4f8f-83e5-24fbd20ab6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiles de ejemplo para probar el código\n",
    "tiles_y = [(-34.1, -33.7), (-34.1, -33.7), (-34.1, -33.7), (-34.1, -33.7), (-34.1, -33.7)]\n",
    "tiles_x = [(-72.0, -71.6), (-71.6, -71.2), (-71.2, -70.8), (-70.8, -70.4), (-70.4, -70.0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7699ebb-7bc4-472e-905a-98f25a96fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar datos de Landsat 5, 7, 8, 9 a DCC\n",
    "dc = datacube.Datacube(app=\"04_Loading_data\")\n",
    "ds = dc.load(product=['landsat5_c2l2_sr', 'landsat7_c2l2_sr', \"landsat8_c2l2_sr\", 'landsat9_c2l2_sr'],\n",
    "             x=(-72.8, -72.4),\n",
    "             y=(-36.5, -36.1),\n",
    "             time=(\"2000-01-01\", \"2023-01-01\"),\n",
    "             output_crs='EPSG:32719',\n",
    "             resolution=(-30,30),\n",
    "             progress_cbk=with_ui_cbk(),\n",
    "             group_by='solar_day',\n",
    "             dask_chunks={\"x\": 2048, \"y\": 2048},\n",
    "             skip_broken_datasets= True\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb0655a-eaea-4905-9146-2401d4c7098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.isel(time=400).red.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2149155f-3c4b-47d3-9a87-e4900bac5f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mascara de nubes\n",
    "cloud_masking = (ds.qa_pixel == 21824) | (ds.qa_pixel == 21952) #clear pixel or terrain occlusion... remove second one if needed\n",
    "ds.update(ds * 0.0000275 + -0.2)\n",
    "ndvi = (ds.nir08 - ds.red) / (ds.nir08  + ds.red ).where(cloud_masking) # for some reason negative values are higher than 1\n",
    "# ndvi = ndvi.where((ndvi >= 0) & (ndvi <= 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26960fb7-55d5-4e9e-bf07-8eeb1937137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi.isel(time=400).plot(vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b708a17-e299-4755-9c8e-0d436a906789",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_error = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d1ba97-7981-42ab-bbf8-239f3775b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = ndvi.chunk({'x':50, 'y':50, 'time':len(ndvi.time.values)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5475c830-5396-4826-ac15-da7aaeb40a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()  # Start measuring the execution time\n",
    "\n",
    "output1 = xr.apply_ufunc(\n",
    "    bkps2dates2,  # Function to apply\n",
    "    chunks,  # Input data\n",
    "    ndvi.time.values,  # Additional arguments\n",
    "    input_core_dims=[['time'], ['time']],  # Dimensions of the input data\n",
    "    output_core_dims=[['new']],  # Dimensions of the output data\n",
    "    exclude_dims=set((\"time\",)),  # Dimensions to exclude from broadcasting\n",
    "    output_sizes={'new':10},  # Sizes of the output dimensions\n",
    "    vectorize=True,  # Vectorize the function for better performance\n",
    "    output_dtypes=[ndvi.dtype],  # Data types of the output\n",
    "    dask='parallelized'  # Use Dask for parallel computation\n",
    ")\n",
    "\n",
    "out = output1.compute()  # Compute the result of the Dask computation\n",
    "\n",
    "end = time.time()  # Stop measuring the execution time\n",
    "print('time', end-start)  # Print the total execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750fe64c-0877-4cfc-906a-cc162637fa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates, bks = out.isel(new=range(5)), out.isel(new=range(5,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b2fd0c-4093-461c-98c1-859d644762e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=ndvi.time.values, data={'ix':range(len(ndvi.time.values))})\n",
    "# sub = ndvi.isel(x=range(30), y=range(30))\n",
    "chunks = ndvi.chunk({'x':50, 'y':50, 'time':len(ndvi.time.values)})\n",
    "# chunks1 = chunks.copy().load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc907511-90b6-43bb-9497-da40ef9ee638",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates.attrs = ds.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adb1451-4810-4b3f-bfad-2c1f2edbd883",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "output1 = xr.apply_ufunc(bkps2dates, \n",
    "                        chunks, \n",
    "                        input_core_dims=[['time']],\n",
    "                        # output_core_dims=[['new']],\n",
    "                        exclude_dims=set((\"time\",)),\n",
    "                        vectorize=True,\n",
    "                        output_dtypes=float,\n",
    "                        # output_sizes={'size':1},\n",
    "                        dask='parallelized')\n",
    "\n",
    "out = output1.compute()\n",
    "end = time.time()\n",
    "print('time', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6ab6b1-294e-4bde-b224-c8150450af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dates = df.reset_index().set_index('ix')\n",
    "vfunc = np.vectorize(get_days(df_dates))\n",
    "days = vfunc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4327e5-621a-4171-b4cc-c2701b2fa93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('tile1.npy', days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e13121-6327-4b76-8b24-353368609f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = {'driver': 'GTiff',\n",
    "        'dtype': 'float32',\n",
    "        'width':dates.shape[1], \n",
    "        'height':dates.shape[0],\n",
    "        'count':5,\n",
    "        'crs':ds.crs,\n",
    "        'transform':ds.affine\n",
    "       }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8366bd8a",
   "metadata": {},
   "source": [
    "### Repetir proceso completo para diferentes zonas o tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e003380-4399-49ad-b415-30f29995ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, n in enumerate(tiles_x[:]):\n",
    "    dc = datacube.Datacube(app=\"04_Loading_data\")\n",
    "    ds = dc.load(product=['landsat5_c2l2_sr', 'landsat7_c2l2_sr'],\n",
    "                 x=n,\n",
    "                 y=tiles_y[i],\n",
    "                 time=(\"2000-01-01\", \"2023-01-01\"),\n",
    "                 output_crs='EPSG:32719',\n",
    "                 resolution=(-30,30),\n",
    "                 progress_cbk=with_ui_cbk(),\n",
    "                 group_by='solar_day',\n",
    "                 dask_chunks={\"x\": 2048, \"y\": 2048},\n",
    "                 skip_broken_datasets= True\n",
    "                )\n",
    "    good_pixel_flags = {\n",
    "        \"snow\": \"not_high_confidence\",\n",
    "        \"cloud\": \"not_high_confidence\",\n",
    "        #  \"cirrus\": \"not_high_confidence\",\n",
    "        \"cloud_shadow\": \"not_high_confidence\",\n",
    "        \"nodata\": False\n",
    "    }\n",
    "    quality_band = 'qa_pixel'\n",
    "    cloud_free_mask1 = masking.make_mask(ds[quality_band], **good_pixel_flags)\n",
    "    ds.update(ds * 0.0000275 + -0.2)\n",
    "    ndvi = (ds.nir08 - ds.red) / (ds.nir08  + ds.red ).where(cloud_free_mask1)\n",
    "\n",
    "    dc = datacube.Datacube(app=\"04_Loading_data\")\n",
    "    ds2 = dc.load(product=[\"landsat8_c2l2_sr\", 'landsat9_c2l2_sr'],\n",
    "                 x=n,\n",
    "                 y=tiles_y[i],\n",
    "                 time=(\"2000-01-01\", \"2023-01-01\"),\n",
    "                 output_crs='EPSG:32719',\n",
    "                 resolution=(-30,30),\n",
    "                 progress_cbk=with_ui_cbk(),\n",
    "                 group_by='solar_day',\n",
    "                 dask_chunks={\"x\": 2048, \"y\": 2048},\n",
    "                 skip_broken_datasets= True\n",
    "                )\n",
    "    good_pixel_flags2 = {\n",
    "        \"snow\": \"not_high_confidence\",\n",
    "        \"cloud\": \"not_high_confidence\",\n",
    "        \"cirrus\": \"not_high_confidence\",\n",
    "        \"cloud_shadow\": \"not_high_confidence\",\n",
    "        \"nodata\": False\n",
    "    }\n",
    "    # quality_band = 'qa_pixel'\n",
    "    cloud_free_mask2 = masking.make_mask(ds2[quality_band], **good_pixel_flags2)\n",
    "    ds2.update(ds2 * 0.0000275 + -0.2)\n",
    "    ndvi2 = (ds2.nir08 - ds2.red) / (ds2.nir08  + ds2.red ).where(cloud_free_mask2)\n",
    "    ndvix = xr.concat([ndvi, ndvi2], dim='time')\n",
    "    # dc = datacube.Datacube(app=\"04_Loading_data\")\n",
    "    # ds = dc.load(product=['landsat5_c2l2_sr', 'landsat7_c2l2_sr', \"landsat8_c2l2_sr\", 'landsat9_c2l2_sr'],\n",
    "    #              x=n,\n",
    "    #              y=tiles_y[labels_error[i]],\n",
    "    #              time=(\"2000-01-01\", \"2023-01-01\"),\n",
    "    #              output_crs='EPSG:32719',\n",
    "    #              resolution=(-30,30),\n",
    "    #              progress_cbk=with_ui_cbk(),\n",
    "    #              group_by='solar_day',\n",
    "    #              dask_chunks={\"x\": 2048, \"y\": 2048},\n",
    "    #              skip_broken_datasets= True\n",
    "    #             )\n",
    "    # cloud_masking = (ds.qa_pixel == 21824) | (ds.qa_pixel == 21952) #clear pixel or terrain occlusion... remove second one if needed\n",
    "    # ds.update(ds * 0.0000275 + -0.2)\n",
    "    # ndvi = (ds.nir08 - ds.red) / (ds.nir08  + ds.red ).where(cloud_masking) # for some reason negative values are higher than 1\n",
    "    chunks = ndvix.chunk({'x':50, 'y':50, 'time':len(ndvix.time.values)})\n",
    "    output1 = xr.apply_ufunc(bkps2dates2, \n",
    "                            chunks, \n",
    "                             ndvix.time.values,\n",
    "                            input_core_dims=[['time'], ['time']],\n",
    "                            output_core_dims=[['new']],\n",
    "                            exclude_dims=set((\"time\",)),\n",
    "                             output_sizes={'new':15},\n",
    "                            vectorize=True,\n",
    "                            output_dtypes=[ndvix.dtype],\n",
    "                            # output_sizes={'size':5},\n",
    "                             # dask='allowed')\n",
    "                            dask='parallelized')\n",
    "\n",
    "    out = output1.compute()\n",
    "    dates, bks, mags = out.isel(new=range(5)), out.isel(new=range(5,10)), out.isel(new=range(10,15))\n",
    "    dates.attrs = ds.attrs\n",
    "    meta = {'driver': 'GTiff',\n",
    "        'dtype': 'float32',\n",
    "        'width':dates.shape[1], \n",
    "        'height':dates.shape[0],\n",
    "        'count':5,\n",
    "        'crs':ds.crs,\n",
    "        'transform':ds.affine\n",
    "       }\n",
    "    with rasterio.open('bks2_1_{}.tif'.format(str(i)), 'w', **meta) as dst:\n",
    "        dst.write(np.moveaxis(dates.values, 2, 0))\n",
    "    with rasterio.open('mag2_1_{}.tif'.format(str(i)), 'w', **meta) as dst:\n",
    "        dst.write(np.moveaxis(mags.values, 2, 0))\n",
    "    del dc, ds, ds2, ndvi, ndvi2, chunks, output1, out, dates, bks, mags, meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678becc1-6c87-4ee9-88b0-e00f40680011",
   "metadata": {},
   "source": [
    "## Proceso similar, pero solo para datos dentro de los poligonos de estudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5704125-9c4f-49f1-8584-e66af23effed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "gdf = gpd.read_file('data/wetlands.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b58c52b-892f-4a94-adde-63d0ca075b39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lw2016 =  [85, 90, 113, 115, 116, 117, 118, 119, 128, 130, 138, 230, 232, 235, 236, 237]\n",
    "keep = [330, 334, 349, 348, 350, 351, 353, 354, 356, 357, 358, 359, 362, 366, 367, 369]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee530fc-0979-449a-945d-e7d1da0f8e0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get unique values in the 'NAME' column of the GeoDataFrame\n",
    "unique_names = gdf['NAME'].unique()\n",
    "\n",
    "# Convert the geometry column of the GeoDataFrame to EPSG:32719 coordinate reference system\n",
    "gdf['geometry'] = gdf['geometry'].to_crs('epsg:32719')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d59db8-b699-478d-9c3b-7131d7b6f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features with NAME 'estable' from the GeoDataFrame\n",
    "estables = gdf[gdf['NAME'] == 'estable']\n",
    "\n",
    "# Select features with NAME 'tala' and exclude features with ID in lw2016, or select features with ID in keep\n",
    "tala = gdf[((gdf['NAME'] == 'tala') & ~(gdf['ID'].isin(lw2016))) | (gdf['ID'].isin(keep))]\n",
    "\n",
    "# Select features with NAME 'sequia' from the GeoDataFrame\n",
    "sequia = gdf[gdf['NAME'] == 'sequia']\n",
    "\n",
    "# Select features with NAME 'incendio' from the GeoDataFrame\n",
    "incendios = gdf[gdf['NAME'] == 'incendio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd70654-86a8-413a-93ee-ca1edb93185f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This code snippet calculates the bounding box coordinates for a given geometry in the GeoDataFrame.\n",
    "\n",
    "name = estables['ID'][1]\n",
    "\n",
    "# Check if the geometry is a MultiPolygon\n",
    "if estables.geometry[n].type == 'MultiPolygon':\n",
    "    # Calculate the bounding box coordinates for each polygon in the MultiPolygon\n",
    "    x0 = np.min([np.min(n.exterior.xy[0]) for n in gdf.geometry[n].geoms])\n",
    "    x1 = np.max([np.max(n.exterior.xy[0]) for n in gdf.geometry[n].geoms])\n",
    "    y0 = np.min([np.min(n.exterior.xy[1]) for n in gdf.geometry[n].geoms])\n",
    "    y1 = np.max([np.max(n.exterior.xy[1]) for n in gdf.geometry[n].geoms])\n",
    "else:\n",
    "    # Calculate the bounding box coordinates for a single polygon\n",
    "    x0, x1 = np.min(gdf.geometry[n].exterior.coords.xy[0]), np.max(gdf.geometry[n].exterior.coords.xy[0])\n",
    "    y0, y1 = np.min(gdf.geometry[n].exterior.coords.xy[1]), np.max(gdf.geometry[n].exterior.coords.xy[1])\n",
    "\n",
    "# Adjust the bounding box coordinates by adding/subtracting a small value\n",
    "x0 = x0 - 0.005\n",
    "x1 = x1 + 0.005\n",
    "y0 = y0 - 0.005\n",
    "y1 = y1 + 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e1aa08",
   "metadata": {},
   "source": [
    "#### Repetir proceso completo sólo en área de polígono"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada7081",
   "metadata": {},
   "source": [
    "Ejemplo para un polígono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ebd497-632e-4045-959d-8b7137859b8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the ID of the first feature in the 'tala' GeoDataFrame\n",
    "name = tala['ID'].iloc[0]\n",
    "print(name)\n",
    "\n",
    "# Calculate the bounding box coordinates for the geometry in the 'tala' GeoDataFrame\n",
    "if tala.geometry.iloc[0].type == 'MultiPolygon':\n",
    "    x0 = np.min([np.min(n.exterior.xy[0]) for n in tala.geometry.iloc[0].geoms])\n",
    "    x1 = np.max([np.max(n.exterior.xy[0]) for n in tala.geometry.iloc[0].geoms])\n",
    "    y0 = np.min([np.min(n.exterior.xy[1]) for n in tala.geometry.iloc[0].geoms])\n",
    "    y1 = np.max([np.max(n.exterior.xy[1]) for n in tala.geometry.iloc[0].geoms])\n",
    "else:\n",
    "    x0, x1 = np.min(tala.geometry.iloc[0].exterior.coords.xy[0]), np.max(tala.geometry.iloc[0].exterior.coords.xy[0])\n",
    "    y0, y1 = np.min(tala.geometry.iloc[0].exterior.coords.xy[1]), np.max(tala.geometry.iloc[0].exterior.coords.xy[1])\n",
    "\n",
    "# Adjust the bounding box coordinates by adding/subtracting a small value\n",
    "x0 = x0 - 0.02\n",
    "x1 = x1 + 0.02\n",
    "y0 = y0 - 0.02\n",
    "y1 = y1 + 0.02\n",
    "\n",
    "# Load satellite data within the specified bounding box coordinates\n",
    "dc = datacube.Datacube(app=\"04_Loading_data\")\n",
    "ds = dc.load(product=['landsat5_c2l2_sr', 'landsat7_c2l2_sr'],\n",
    "             x=(x0, x1),\n",
    "             y=(y0, y1),\n",
    "             time=(\"2000-01-01\", \"2023-01-01\"),\n",
    "             output_crs='EPSG:32719',\n",
    "             resolution=(-30,30),\n",
    "             progress_cbk=with_ui_cbk(),\n",
    "             group_by='solar_day',\n",
    "             dask_chunks={\"x\": 2048, \"y\": 2048},\n",
    "             skip_broken_datasets= True\n",
    "            )\n",
    "good_pixel_flags = {\n",
    "    \"snow\": \"not_high_confidence\",\n",
    "    \"cloud\": \"not_high_confidence\",\n",
    "    #  \"cirrus\": \"not_high_confidence\",\n",
    "    \"cloud_shadow\": \"not_high_confidence\",\n",
    "    \"nodata\": False\n",
    "}\n",
    "quality_band = 'qa_pixel'\n",
    "cloud_free_mask1 = masking.make_mask(ds[quality_band], **good_pixel_flags)\n",
    "ds.update(ds * 0.0000275 + -0.2)\n",
    "ndvi = (ds.nir08 - ds.red) / (ds.nir08  + ds.red ).where(cloud_free_mask1)\n",
    "\n",
    "dc = datacube.Datacube(app=\"04_Loading_data\")\n",
    "ds2 = dc.load(product=[\"landsat8_c2l2_sr\", 'landsat9_c2l2_sr'],\n",
    "             x=(x0, x1),\n",
    "             y=(y0, y1),\n",
    "             time=(\"2000-01-01\", \"2023-01-01\"),\n",
    "             output_crs='EPSG:32719',\n",
    "             resolution=(-30,30),\n",
    "             progress_cbk=with_ui_cbk(),\n",
    "             group_by='solar_day',\n",
    "             dask_chunks={\"x\": 2048, \"y\": 2048},\n",
    "             skip_broken_datasets= True\n",
    "            )\n",
    "good_pixel_flags2 = {\n",
    "    \"snow\": \"not_high_confidence\",\n",
    "    \"cloud\": \"not_high_confidence\",\n",
    "    \"cirrus\": \"not_high_confidence\",\n",
    "    \"cloud_shadow\": \"not_high_confidence\",\n",
    "    \"nodata\": False\n",
    "}\n",
    "\n",
    "cloud_free_mask2 = masking.make_mask(ds2[quality_band], **good_pixel_flags2)\n",
    "ds2.update(ds2 * 0.0000275 + -0.2)\n",
    "ndvi2 = (ds2.nir08 - ds2.red) / (ds2.nir08  + ds2.red ).where(cloud_free_mask2)\n",
    "ndvix = xr.concat([ndvi, ndvi2], dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6df327-2997-4794-b068-f06c563a9bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(ndvix.time.values, ndvix.isel(x=5, y=5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a295f740-ea36-49cf-ad0d-9a82095868de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ndvix.isel(time=100).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9513aed8-8286-4cf7-8efc-634fefd7a8b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunks = ndvix.chunk({'x':50, 'y':50, 'time':len(ndvix.time.values)})\n",
    "output1 = xr.apply_ufunc(bkps2dates2, \n",
    "                         chunks, \n",
    "                         ndvix.time.values,\n",
    "                         input_core_dims=[['time'], ['time']],\n",
    "                         output_core_dims=[['new']],\n",
    "                         exclude_dims=set((\"time\",)),\n",
    "                         output_sizes={'new':15},\n",
    "                         vectorize=True,\n",
    "                         output_dtypes=[ndvix.dtype],\n",
    "                        # output_sizes={'size':5},\n",
    "                         # dask='allowed')\n",
    "                         dask='parallelized')\n",
    "\n",
    "out = output1.compute()\n",
    "dates, bks, mags = out.isel(new=range(5)), out.isel(new=range(5,10)), out.isel(new=range(10,15))\n",
    "dates.attrs = ds.attrs\n",
    "meta = {'driver': 'GTiff',\n",
    "        'dtype': 'float32',\n",
    "        'width':dates.shape[1], \n",
    "        'height':dates.shape[0],\n",
    "        'count':5,\n",
    "        'crs':ds.crs,\n",
    "        'transform':ds.affine\n",
    "       }\n",
    "with rasterio.open('geo_bkstala_{}.tif'.format('test3'), 'w', **meta) as dst:\n",
    "    dst.write(np.moveaxis(dates.values, 2, 0))\n",
    "with rasterio.open('geo_magtala_{}.tif'.format('test3'), 'w', **meta) as dst:\n",
    "    dst.write(np.moveaxis(mags.values, 2, 0))\n",
    "del dc, ds, ds2, cloud_free_mask1, cloud_free_mask2, ndvi, ndvi2, ndvix, chunks, output1, out, dates, bks, mags, meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f289818",
   "metadata": {},
   "source": [
    "Loop para todos los polígonos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b907e-5b14-4548-9634-4b942fcf4e55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for n in range(len(incendios))[52:]:\n",
    "    name = incendios['ID'].iloc[n]\n",
    "    if incendios.geometry.iloc[n].type == 'MultiPolygon':\n",
    "        x0 = np.min([np.min(n.exterior.xy[0]) for n in incendios.geometry.iloc[n].geoms])\n",
    "        x1 = np.max([np.max(n.exterior.xy[0]) for n in incendios.geometry.iloc[n].geoms])\n",
    "        y0 = np.min([np.min(n.exterior.xy[1]) for n in incendios.geometry.iloc[n].geoms])\n",
    "        y1 = np.max([np.max(n.exterior.xy[1]) for n in incendios.geometry.iloc[n].geoms])\n",
    "    else:\n",
    "        x0, x1 = np.min(incendios.geometry.iloc[n].exterior.coords.xy[0]), np.max(incendios.geometry.iloc[n].exterior.coords.xy[0])\n",
    "        y0, y1 = np.min(incendios.geometry.iloc[n].exterior.coords.xy[1]), np.max(incendios.geometry.iloc[n].exterior.coords.xy[1])\n",
    "    x0 = x0 - 0.02\n",
    "    x1 = x1 + 0.02\n",
    "    y0 = y0 - 0.02\n",
    "    y1 = y1 + 0.02\n",
    "    dc = datacube.Datacube(app=\"04_Loading_data\")\n",
    "    ds = dc.load(product=['landsat5_c2l2_sr', 'landsat7_c2l2_sr'],\n",
    "                 x=(x0, x1),\n",
    "                 y=(y0, y1),\n",
    "                 time=(\"2000-01-01\", \"2023-01-01\"),\n",
    "                 output_crs='EPSG:32719',\n",
    "                 resolution=(-30,30),\n",
    "                 progress_cbk=with_ui_cbk(),\n",
    "                 group_by='solar_day',\n",
    "                 dask_chunks={\"x\": 2048, \"y\": 2048},\n",
    "                 skip_broken_datasets= True\n",
    "                )\n",
    "    good_pixel_flags = {\n",
    "        \"snow\": \"not_high_confidence\",\n",
    "        \"cloud\": \"not_high_confidence\",\n",
    "        #  \"cirrus\": \"not_high_confidence\",\n",
    "        \"cloud_shadow\": \"not_high_confidence\",\n",
    "        \"nodata\": False\n",
    "    }\n",
    "    quality_band = 'qa_pixel'\n",
    "    cloud_free_mask1 = masking.make_mask(ds[quality_band], **good_pixel_flags)\n",
    "    ds.update(ds * 0.0000275 + -0.2)\n",
    "    ndvi = (ds.nir08 - ds.red) / (ds.nir08  + ds.red ).where(cloud_free_mask1)\n",
    "    \n",
    "    dc = datacube.Datacube(app=\"04_Loading_data\")\n",
    "    ds2 = dc.load(product=[\"landsat8_c2l2_sr\", 'landsat9_c2l2_sr'],\n",
    "                 x=(x0, x1),\n",
    "                 y=(y0, y1),\n",
    "                 time=(\"2000-01-01\", \"2023-01-01\"),\n",
    "                 output_crs='EPSG:32719',\n",
    "                 resolution=(-30,30),\n",
    "                 progress_cbk=with_ui_cbk(),\n",
    "                 group_by='solar_day',\n",
    "                 dask_chunks={\"x\": 2048, \"y\": 2048},\n",
    "                 skip_broken_datasets= True\n",
    "                )\n",
    "    good_pixel_flags2 = {\n",
    "        \"snow\": \"not_high_confidence\",\n",
    "        \"cloud\": \"not_high_confidence\",\n",
    "        \"cirrus\": \"not_high_confidence\",\n",
    "        \"cloud_shadow\": \"not_high_confidence\",\n",
    "        \"nodata\": False\n",
    "    }\n",
    "\n",
    "    cloud_free_mask2 = masking.make_mask(ds2[quality_band], **good_pixel_flags2)\n",
    "    ds2.update(ds2 * 0.0000275 + -0.2)\n",
    "    ndvi2 = (ds2.nir08 - ds2.red) / (ds2.nir08  + ds2.red ).where(cloud_free_mask2)\n",
    "    ndvix = xr.concat([ndvi, ndvi2], dim='time')\n",
    "    \n",
    "    chunks = ndvix.chunk({'x':50, 'y':50, 'time':len(ndvix.time.values)})\n",
    "    output1 = xr.apply_ufunc(bkps2dates2, \n",
    "                             chunks, \n",
    "                             ndvix.time.values,\n",
    "                             input_core_dims=[['time'], ['time']],\n",
    "                             output_core_dims=[['new']],\n",
    "                             exclude_dims=set((\"time\",)),\n",
    "                             output_sizes={'new':15},\n",
    "                             vectorize=True,\n",
    "                             output_dtypes=[ndvix.dtype],\n",
    "                            # output_sizes={'size':5},\n",
    "                             # dask='allowed')\n",
    "                             dask='parallelized')\n",
    "\n",
    "    out = output1.compute()\n",
    "    dates, bks, mags = out.isel(new=range(5)), out.isel(new=range(5,10)), out.isel(new=range(10,15))\n",
    "    dates.attrs = ds.attrs\n",
    "    meta = {'driver': 'GTiff',\n",
    "            'dtype': 'float32',\n",
    "            'width':dates.shape[1], \n",
    "            'height':dates.shape[0],\n",
    "            'count':5,\n",
    "            'crs':ds.crs,\n",
    "            'transform':ds.affine\n",
    "           }\n",
    "    with rasterio.open('geo_bksincendios_{}.tif'.format(n), 'w', **meta) as dst:\n",
    "        dst.write(np.moveaxis(dates.values, 2, 0))\n",
    "    with rasterio.open('geo_magincendios_{}.tif'.format(n), 'w', **meta) as dst:\n",
    "        dst.write(np.moveaxis(mags.values, 2, 0))\n",
    "    del dc, ds, ds2, cloud_free_mask1, cloud_free_mask2, ndvi, ndvi2, ndvix, chunks, output1, out, dates, bks, mags, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37bfc49-a61c-4a65-8383-8484a93f7158",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output1.isel(new=0).plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
